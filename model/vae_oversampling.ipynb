{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f147c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import collections\n",
    "import numpy as np \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "226c5cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vae_model(tf.keras.Model):\n",
    "    def __init__(self,z_dim,input_dim,hidden_num=2):\n",
    "        super(vae_model, self).__init__()\n",
    "        self.input_dim=input_dim\n",
    "        self.z_dim=z_dim\n",
    "        self.hidden_num=hidden_num#选择几层编码层，1表示只用64,2表示用[64,128]，3表示只用128\n",
    "        self.encoder_f1=tf.keras.layers.Dense(128,activation='relu')\n",
    "        self.encoder_f2=tf.keras.layers.Dense(64,activation='relu')\n",
    "        self.encoder_f3_mean=tf.keras.layers.Dense(self.z_dim)#求均值的层\n",
    "        self.encoder_f3_var=tf.keras.layers.Dense(self.z_dim)#求var的层\n",
    "        \n",
    "        self.decoder_f1=tf.keras.layers.Dense(128,activation='relu')\n",
    "        self.decoder_f2=tf.keras.layers.Dense(64,activation='relu')\n",
    "        self.decoder_f3=tf.keras.layers.Dense(self.input_dim,activation='relu')\n",
    "\n",
    "    # encoder传播的过程\n",
    "    def encoder(self, x):\n",
    "        if self.hidden_num==3:\n",
    "            h=self.encoder_f1(x)\n",
    "            mu=self.encoder_f3_mean(h)\n",
    "            log_var=self.encoder_f3_var(h)\n",
    "        elif self.hidden_num==2:\n",
    "            h=self.encoder_f1(x)\n",
    "            h=self.encoder_f2(h)\n",
    "            mu=self.encoder_f3_mean(h)\n",
    "            log_var=self.encoder_f3_var(h)\n",
    "        else:\n",
    "            h=self.encoder_f2(x)\n",
    "            mu=self.encoder_f3_mean(h)\n",
    "            log_var=self.encoder_f3_var(h)\n",
    "\n",
    "        return  mu, log_var\n",
    "\n",
    "    # decoder传播的过程\n",
    "    def decoder(self, z):\n",
    "        if self.hidden_num==3:\n",
    "            h=self.decoder_f1(z)\n",
    "            output=self.decoder_f3(h)\n",
    "        elif self.hidden_num==2:\n",
    "            h=self.decoder_f2(z)\n",
    "            h=self.decoder_f1(h)\n",
    "            output=self.decoder_f3(h)\n",
    "        else:\n",
    "            h=self.decoder_f2(z)\n",
    "            output=self.decoder_f3(h)\n",
    "        return output\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        eps = tf.random.normal(log_var.shape)\n",
    "        std = tf.exp(log_var)         # 去掉log, 得到方差；\n",
    "        std = std**0.5                # 开根号，得到标准差；\n",
    "        z = mu + std * eps\n",
    "        return z\n",
    "\n",
    "    def call(self, inputs,training=None):\n",
    "        mu, log_var = self.encoder(inputs)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat, mu, log_var\n",
    "\n",
    "class VAE():\n",
    "    def __init__(self,data_path,z_dim=2,max_epochs=1000,batch_size=128,patient_num=50,hidden_num=1,lr=1e-4,delta=1e-4):\n",
    "#         super(VAE,self).__init__()\n",
    "        self.data_path=data_path\n",
    "        self.max_epochs=max_epochs\n",
    "        self.batch_size=batch_size\n",
    "        self.patient_num=patient_num\n",
    "        self.z_dim=z_dim\n",
    "        self.hidden_num=hidden_num\n",
    "        self.lr=lr\n",
    "        self.delta=delta#验证集误差低于该值时，迭代停止\n",
    "        \n",
    "        self.load_data()        \n",
    "        self.input_dim=self.data_label_1_x.shape[1]\n",
    "        \n",
    "        #定义VAE的结构\n",
    "        self.vae_model=vae_model(self.z_dim,self.input_dim,self.hidden_num)\n",
    "#         self.vae_model.build(input_shape=(self.batch_size, self.input_dim))\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        加载数据，得到少数类数据\n",
    "        \"\"\"\n",
    "        self.data=pd.read_csv(self.data_path)\n",
    "        self.label_num_dict=dict(collections.Counter(self.data['label']))\n",
    "        self.label_0_num=self.label_num_dict[0]\n",
    "        self.label_1_num=self.label_num_dict[1]\n",
    "        self.need_add_label_num=self.label_0_num-self.label_1_num#需要增强的数据量\n",
    "        self.data_label_0=self.data[self.data['label']==0]\n",
    "        self.data_label_1=self.data[self.data['label']==1]\n",
    "        \n",
    "        self.data_label_1_x=self.data_label_1[self.data_label_1.columns[:-1]]\n",
    "        self.data_label_1_y=self.data_label_1[self.data_label_1.columns[-1:]]\n",
    "        \n",
    "        #数据归一化处理\n",
    "        self.scale_process()\n",
    "        \n",
    "        #数据打包\n",
    "        self.train_x,self.train_y,self.train_dataset=self.zip_data(self.train_x,self.train_y)\n",
    "        self.valid_x,self.valid_y,self.valid_dataset=self.zip_data(self.valid_x,self.valid_y)\n",
    "        self.test_x,self.test_y,self.test_dataset=self.zip_data(self.test_x,self.test_y)\n",
    "\n",
    "    def scale_process(self):\n",
    "        \"\"\"\n",
    "        数据归一化处理\n",
    "        数据切分\n",
    "        \"\"\"\n",
    "        self.maxabs = preprocessing.MaxAbsScaler()\n",
    "        self.data_label_1_x = self.maxabs.fit_transform(self.data_label_1_x)\n",
    "        self.train_x,self.test_x,self.train_y,self.test_y=train_test_split(self.data_label_1_x,self.data_label_1_y,test_size=0.4)\n",
    "        self.valid_x,self.test_x,self.valid_y,self.test_y=train_test_split(self.test_x,self.test_y,test_size=0.5)\n",
    "\n",
    "    def zip_data(self,data_x,data_y):\n",
    "        \"\"\"\n",
    "        以batch_size的大小将数据打包\n",
    "        \"\"\"\n",
    "        train_data = tf.data.Dataset.from_tensor_slices(data_x).batch(self.batch_size)\n",
    "        train_labels = tf.data.Dataset.from_tensor_slices(data_y.values[:,0]).batch(self.batch_size)\n",
    "        train_dataset = tf.data.Dataset.zip((train_data,train_labels)).shuffle(True)\n",
    "        return train_data,train_data,train_dataset\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        \"\"\"\n",
    "        global x_hat,x,mu, log_var,loss\n",
    "        #创建模型参数\n",
    "        self.vae_model.build(input_shape=(self.batch_size, self.input_dim))\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=self.lr)\n",
    "        self.valid_loss_list=[]#记录每个epoch的验证集损失值，用于早停机制中\n",
    "        self.smallest_loss=np.inf\n",
    "        self.train_mu_list=[]\n",
    "        self.train_logvar_list=[]\n",
    "        \n",
    "        for epoch in range(self.max_epochs):\n",
    "            epoch_loss=0\n",
    "            data_num=0\n",
    "            for x,y in self.train_dataset:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    x_hat, mu, log_var=self.vae_model(x)\n",
    "                    rec_loss = tf.reduce_mean(tf.losses.MSE(x, x_hat))\n",
    "                    kl_div = -0.5 * (log_var + 1 -mu**2 - tf.exp(log_var))\n",
    "                    kl_div = tf.reduce_mean(kl_div) / x.shape[0]\n",
    "                    \n",
    "                    #误差求和\n",
    "                    loss = rec_loss + 1. * kl_div\n",
    "                \n",
    "                grads = tape.gradient(loss, self.vae_model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.vae_model.trainable_variables))\n",
    "                epoch_loss+=loss.numpy()*x.shape[0]\n",
    "                data_num+=x.shape[0]\n",
    "                \n",
    "                self.train_mu_list+=mu.numpy().tolist()\n",
    "                self.train_logvar_list+=log_var.numpy().tolist()\n",
    "            \n",
    "            \n",
    "            #得到验证集的损失值，用于判断是否需要早停\n",
    "            valid_loss=self.evaluate_loss()\n",
    "            self.valid_loss_list+=[valid_loss]\n",
    "            print(f'epoch={epoch},train_loss={epoch_loss/data_num},valid_loss={valid_loss}')\n",
    "            \n",
    "            if valid_loss<self.smallest_loss:\n",
    "                self.best_vae_model=self.vae_model\n",
    "                \n",
    "            if self.patient_num!=None:\n",
    "                #需要使用早停机制\n",
    "                min_loss_index=np.argmin(self.valid_loss_list)#验证集损失值最小的epoch处\n",
    "                if len(self.valid_loss_list)-min_loss_index>self.patient_num or valid_loss<self.delta:\n",
    "                    #达到早停结束的时刻\n",
    "                    break                \n",
    "            \n",
    "    def evaluate_loss(self):\n",
    "        \"\"\"\n",
    "        计算验证集的损失值\n",
    "        \"\"\"\n",
    "        epoch_loss=0\n",
    "        data_num=0\n",
    "        for x,y in self.valid_dataset:\n",
    "            x_hat, mu, log_var=self.vae_model(x)\n",
    "            rec_loss = tf.reduce_mean(tf.losses.MSE(x, x_hat))\n",
    "            kl_div = -0.5 * (log_var + 1 -mu**2 - tf.exp(log_var))\n",
    "            kl_div = tf.reduce_mean(kl_div) / x.shape[0]\n",
    "            loss = rec_loss + 1. * kl_div\n",
    "            \n",
    "            epoch_loss+=loss.numpy()*x.shape[0]\n",
    "            data_num+=x.shape[0]\n",
    "        return epoch_loss/data_num\n",
    "    \n",
    "    def resample(self,creat_num=None):\n",
    "        #生成z\n",
    "        if creat_num==None or creat_num<=0:\n",
    "            new_creat_z=np.random.normal(size=(self.need_add_label_num,self.z_dim))\n",
    "        else:\n",
    "            new_creat_z=np.random.normal(size=(creat_num,self.z_dim))\n",
    "        #生成x\n",
    "        new_creat_x_normal=self.best_vae_model.decoder(new_creat_z).numpy()\n",
    "        #再反归一化\n",
    "        new_creat_x=self.maxabs.inverse_transform(new_creat_x_normal)\n",
    "        new_creat_x=pd.DataFrame(new_creat_x,columns=self.data_label_1.columns[:-1])\n",
    "        new_creat_y=pd.DataFrame([1]*len(new_creat_x),columns=['label'])\n",
    "        return new_creat_x,new_creat_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d890bcf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creat data:  prosper(100)\n",
      "epoch=0,train_loss=0.19635505020618438,valid_loss=0.19708271324634552\n",
      "epoch=1,train_loss=0.19713238060474395,valid_loss=0.19443483650684357\n",
      "epoch=2,train_loss=0.19755476593971252,valid_loss=0.1971963793039322\n",
      "epoch=3,train_loss=0.19325916945934296,valid_loss=0.19386297464370728\n",
      "epoch=4,train_loss=0.1939113885164261,valid_loss=0.19560697674751282\n",
      "epoch=5,train_loss=0.19600453197956086,valid_loss=0.19250980019569397\n",
      "epoch=6,train_loss=0.19348806977272034,valid_loss=0.19267240166664124\n",
      "epoch=7,train_loss=0.1926753932237625,valid_loss=0.18982240557670593\n",
      "epoch=8,train_loss=0.19250185847282408,valid_loss=0.19221898913383484\n",
      "epoch=9,train_loss=0.19230149686336517,valid_loss=0.19131238758563995\n",
      "epoch=10,train_loss=0.18995550215244295,valid_loss=0.19014395773410797\n",
      "epoch=11,train_loss=0.1892388367652893,valid_loss=0.188225656747818\n",
      "epoch=12,train_loss=0.18805033802986146,valid_loss=0.1889350563287735\n",
      "epoch=13,train_loss=0.18879729866981507,valid_loss=0.1881318837404251\n",
      "epoch=14,train_loss=0.18796782791614533,valid_loss=0.1877889335155487\n",
      "epoch=15,train_loss=0.18675200700759886,valid_loss=0.1828594207763672\n",
      "epoch=16,train_loss=0.18487302482128143,valid_loss=0.18819859623908997\n",
      "epoch=17,train_loss=0.18546424329280853,valid_loss=0.1854313462972641\n",
      "epoch=18,train_loss=0.18446144461631775,valid_loss=0.18115811049938202\n",
      "epoch=19,train_loss=0.1835951203107834,valid_loss=0.18459771573543549\n",
      "epoch=20,train_loss=0.18081742346286775,valid_loss=0.1796504110097885\n",
      "epoch=21,train_loss=0.18181803703308105,valid_loss=0.18255041539669037\n",
      "epoch=22,train_loss=0.1820501434803009,valid_loss=0.18147750198841095\n",
      "epoch=23,train_loss=0.18135952234268188,valid_loss=0.18051758408546448\n",
      "epoch=24,train_loss=0.1803453516960144,valid_loss=0.1754755824804306\n",
      "epoch=25,train_loss=0.17842282772064208,valid_loss=0.177154541015625\n",
      "epoch=26,train_loss=0.17727986216545105,valid_loss=0.17492781579494476\n",
      "epoch=27,train_loss=0.1770848512649536,valid_loss=0.17544326186180115\n",
      "epoch=28,train_loss=0.17702843904495238,valid_loss=0.17717471718788147\n",
      "epoch=29,train_loss=0.17623886346817016,valid_loss=0.17566253244876862\n",
      "epoch=30,train_loss=0.1743285584449768,valid_loss=0.17462410032749176\n",
      "epoch=31,train_loss=0.174414541721344,valid_loss=0.17464813590049744\n",
      "epoch=32,train_loss=0.17165836155414582,valid_loss=0.1747560054063797\n",
      "epoch=33,train_loss=0.1704882538318634,valid_loss=0.17175441980361938\n",
      "epoch=34,train_loss=0.171461541056633,valid_loss=0.1744934767484665\n",
      "epoch=35,train_loss=0.1698932820558548,valid_loss=0.16895751655101776\n",
      "epoch=36,train_loss=0.17034429848194121,valid_loss=0.16841191053390503\n",
      "epoch=37,train_loss=0.16731247007846833,valid_loss=0.1696668565273285\n",
      "epoch=38,train_loss=0.16758160591125487,valid_loss=0.1678524613380432\n",
      "epoch=39,train_loss=0.1671486908197403,valid_loss=0.16919651627540588\n",
      "epoch=40,train_loss=0.1686963778734207,valid_loss=0.16722653806209564\n",
      "epoch=41,train_loss=0.1665852564573288,valid_loss=0.1665276437997818\n",
      "epoch=42,train_loss=0.1665179920196533,valid_loss=0.1654580682516098\n",
      "epoch=43,train_loss=0.16275621235370635,valid_loss=0.16676951944828033\n",
      "epoch=44,train_loss=0.16281309843063355,valid_loss=0.16599567234516144\n",
      "epoch=45,train_loss=0.1601032280921936,valid_loss=0.1630876362323761\n",
      "epoch=46,train_loss=0.16137847900390626,valid_loss=0.1624840795993805\n",
      "epoch=47,train_loss=0.1589772307872772,valid_loss=0.16113132238388062\n",
      "epoch=48,train_loss=0.15719610452651978,valid_loss=0.15852199494838715\n",
      "epoch=49,train_loss=0.15673539996147157,valid_loss=0.16020186245441437\n",
      "epoch=50,train_loss=0.158698348402977,valid_loss=0.15716663002967834\n",
      "epoch=51,train_loss=0.15715552270412445,valid_loss=0.15760724246501923\n",
      "epoch=52,train_loss=0.15556647181510924,valid_loss=0.1585206836462021\n",
      "epoch=53,train_loss=0.15465494573116303,valid_loss=0.15742971003055573\n",
      "epoch=54,train_loss=0.1535835897922516,valid_loss=0.15513673424720764\n",
      "epoch=55,train_loss=0.15106835782527925,valid_loss=0.15346726775169373\n",
      "epoch=56,train_loss=0.15036494970321657,valid_loss=0.15353438258171082\n",
      "epoch=57,train_loss=0.14916181862354277,valid_loss=0.1511201709508896\n",
      "epoch=58,train_loss=0.1512116724252701,valid_loss=0.1500869244337082\n",
      "epoch=59,train_loss=0.14673638343811035,valid_loss=0.15151701867580414\n",
      "epoch=60,train_loss=0.1465645134449005,valid_loss=0.14352577924728394\n",
      "epoch=61,train_loss=0.14289808988571168,valid_loss=0.1465972363948822\n",
      "epoch=62,train_loss=0.14858993232250214,valid_loss=0.14493142068386078\n",
      "epoch=63,train_loss=0.14358596205711366,valid_loss=0.14087720215320587\n",
      "epoch=64,train_loss=0.1441116625070572,valid_loss=0.14843127131462097\n",
      "epoch=65,train_loss=0.14014389097690583,valid_loss=0.1470343917608261\n",
      "epoch=66,train_loss=0.14206827878952027,valid_loss=0.14170102775096893\n",
      "epoch=67,train_loss=0.1401226341724396,valid_loss=0.14187325537204742\n",
      "epoch=68,train_loss=0.1406581473350525,valid_loss=0.13895779848098755\n",
      "epoch=69,train_loss=0.13916923761367797,valid_loss=0.1376591920852661\n",
      "epoch=70,train_loss=0.13709728717803954,valid_loss=0.13952086865901947\n",
      "epoch=71,train_loss=0.13642644107341767,valid_loss=0.13668759167194366\n",
      "epoch=72,train_loss=0.1326347053050995,valid_loss=0.13561995327472687\n",
      "epoch=73,train_loss=0.13184175193309783,valid_loss=0.13769303262233734\n",
      "epoch=74,train_loss=0.133704035282135,valid_loss=0.1311037242412567\n",
      "epoch=75,train_loss=0.12920380294322967,valid_loss=0.13480211794376373\n",
      "epoch=76,train_loss=0.13064177036285402,valid_loss=0.13062618672847748\n",
      "epoch=77,train_loss=0.12913931667804718,valid_loss=0.12948574125766754\n",
      "epoch=78,train_loss=0.12816977858543396,valid_loss=0.13368354737758636\n",
      "epoch=79,train_loss=0.12626088976860048,valid_loss=0.13110129535198212\n",
      "epoch=80,train_loss=0.12777179419994356,valid_loss=0.12747594714164734\n",
      "epoch=81,train_loss=0.12183532357215882,valid_loss=0.12965057790279388\n",
      "epoch=82,train_loss=0.12112025082111359,valid_loss=0.12680406868457794\n",
      "epoch=83,train_loss=0.12284970343112946,valid_loss=0.12405683845281601\n",
      "epoch=84,train_loss=0.11903653562068939,valid_loss=0.12452802062034607\n",
      "epoch=85,train_loss=0.11924314141273498,valid_loss=0.11931294947862625\n",
      "epoch=86,train_loss=0.11596916615962982,valid_loss=0.12145442515611649\n",
      "epoch=87,train_loss=0.11714999884366989,valid_loss=0.1214800477027893\n",
      "epoch=88,train_loss=0.11535592079162597,valid_loss=0.1172049269080162\n",
      "epoch=89,train_loss=0.1156146439909935,valid_loss=0.1137615367770195\n",
      "epoch=90,train_loss=0.11288465052843094,valid_loss=0.11516804993152618\n",
      "epoch=91,train_loss=0.11379614144563675,valid_loss=0.11152927577495575\n",
      "epoch=92,train_loss=0.11290857285261154,valid_loss=0.1122431680560112\n",
      "epoch=93,train_loss=0.11012129217386246,valid_loss=0.11154287308454514\n",
      "epoch=94,train_loss=0.10915174186229706,valid_loss=0.1129801943898201\n",
      "epoch=95,train_loss=0.10857795476913452,valid_loss=0.10684504359960556\n",
      "epoch=96,train_loss=0.10881114274263382,valid_loss=0.11124671995639801\n",
      "epoch=97,train_loss=0.10537520617246628,valid_loss=0.1132764220237732\n",
      "epoch=98,train_loss=0.10406347066164016,valid_loss=0.1097443550825119\n",
      "epoch=99,train_loss=0.10528576493263245,valid_loss=0.10845142602920532\n",
      "creat data:  gcdata\n",
      "epoch=0,train_loss=0.30549816025627985,valid_loss=0.30017635226249695\n",
      "epoch=1,train_loss=0.30681187974082097,valid_loss=0.2980121672153473\n",
      "epoch=2,train_loss=0.2991205804877811,valid_loss=0.2979300320148468\n",
      "epoch=3,train_loss=0.3003628644678328,valid_loss=0.30808645486831665\n",
      "epoch=4,train_loss=0.30163770318031313,valid_loss=0.294974148273468\n",
      "epoch=5,train_loss=0.29759035176701015,valid_loss=0.30005258321762085\n",
      "epoch=6,train_loss=0.3001643743779924,valid_loss=0.29698601365089417\n",
      "epoch=7,train_loss=0.29953210883670384,valid_loss=0.2933969795703888\n",
      "epoch=8,train_loss=0.2953173326121436,valid_loss=0.2931526005268097\n",
      "epoch=9,train_loss=0.29082969162199235,valid_loss=0.2833338677883148\n",
      "epoch=10,train_loss=0.2931737926271227,valid_loss=0.30037882924079895\n",
      "epoch=11,train_loss=0.2940139863226149,valid_loss=0.2910437285900116\n",
      "epoch=12,train_loss=0.2931402848826514,valid_loss=0.294147253036499\n",
      "epoch=13,train_loss=0.294284360938602,valid_loss=0.292354017496109\n",
      "epoch=14,train_loss=0.29311983121765983,valid_loss=0.29145848751068115\n"
     ]
    }
   ],
   "source": [
    "for data_name in ['prosper(100)','gcdata']:\n",
    "    print('creat data: ',data_name)\n",
    "    data_path=f'./{data_name}.csv'\n",
    "    vae=VAE(data_path,z_dim=4,max_epochs=100,batch_size=128,patient_num=5,hidden_num=1,lr=1e-4,delta=1e-4)\n",
    "    vae.fit()\n",
    "    new_sample_data_x,new_sample_data_y=vae.resample()\n",
    "    new_sample_data=new_sample_data_x.T.append(new_sample_data_y.T).T\n",
    "    if os.path.exists('./new_data/')==False:\n",
    "        os.makedirs('./new_data/')\n",
    "    new_sample_data.to_csv(f'./new_data/{data_name}.csv',encoding='utf_8_sig',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad56085",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
